{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14cbb4c5",
   "metadata": {},
   "source": [
    "### Version simple (compter le nombre de mots de l'offre dans le CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4a0eebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eupho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Téléchargement des stopwords français si nécessaire\n",
    "nltk.download('stopwords')\n",
    "stop_fr = set(stopwords.words('french'))\n",
    "\n",
    "def lire_docx(path):\n",
    "    \"\"\"Extrait le texte brut d'un fichier Word (.docx).\"\"\"\n",
    "    doc = Document(path)\n",
    "    full_text = [p.text for p in doc.paragraphs]\n",
    "    return \"\\n\".join(full_text)\n",
    "\n",
    "# --- Nettoyage de texte ---\n",
    "def nettoyer_texte(texte: str) -> str:\n",
    "    \"\"\"Met en minuscules et supprime caractères spéciaux.\"\"\"\n",
    "    texte = texte.lower()\n",
    "    texte = re.sub(r\"[^a-zàâçéèêëîïôûùüÿñæœ0-9\\s]\", \" \", texte)\n",
    "    texte = re.sub(r\"\\s+\", \" \", texte)\n",
    "    return texte.strip()\n",
    "\n",
    "# --- Extraction des mots clés d'une offre (stop words exclus) ---\n",
    "def extraire_mots_cles_offre(offre_text: str):\n",
    "    \"\"\"\n",
    "    Retourne la liste des mots clés uniques de l'offre après nettoyage\n",
    "    et suppression des stop words français.\n",
    "    \"\"\"\n",
    "    texte = nettoyer_texte(offre_text)\n",
    "    mots = texte.split()\n",
    "    # Exclusion des stop words\n",
    "    mots_cles = [mot for mot in mots if mot not in stop_fr]\n",
    "    return list(set(mots_cles))  # mots uniques\n",
    "\n",
    "# --- Score de correspondance CV vs offre ---\n",
    "def score_cv_offre(cv_text: str, offre_text: str):\n",
    "    \"\"\"\n",
    "    Calcule le pourcentage de mots-clés de l'offre présents dans le CV.\n",
    "    \"\"\"\n",
    "    cv_clean = nettoyer_texte(cv_text)\n",
    "    cv_mots = set(cv_clean.split())\n",
    "\n",
    "    mots_cles = extraire_mots_cles_offre(offre_text)\n",
    "    # print(\"mots_cles:\", mots_cles)\n",
    "\n",
    "    nb_trouves = sum(1 for mot in mots_cles if mot in cv_mots)\n",
    "    score = nb_trouves / len(mots_cles) if mots_cles else 0\n",
    "\n",
    "    return score * 100  # score en pourcentage\n",
    "\n",
    "offre = \"\"\"\n",
    "Dans le cadre de sa mission d’exploitation et de valorisation des données médicales, la DIDM fait face à un besoin croissant de données fiables. C’est pourquoi un nouveau poste est créé.\n",
    "Vous viendrez compléter une équipe composée d’une Chargée d’études et développements à 50 % et d’un Responsable Etudes et Développements. Sous la responsabilité de ce dernier, vos missions seront les suivantes :\n",
    "Construire des pipelines de données pour alimenter la BI et l’analytique.\n",
    "Modéliser et structurer les flux, tables et schémas\n",
    "Garantir la qualité, la fiabilité et la sécurité des données\n",
    "Développer de nouveaux datasets pour la BI de la DIDM\n",
    "Mettre en place des standards de développement et de bonnes pratiques\n",
    "Assurer le support et la résolution des incidents sur votre périmètre...\n",
    "\n",
    "Votre boîte à outils\n",
    "Excellente maîtrise de SQL (Oracle) et solide expérience en R\n",
    "Connaissances en Julia, Java ou Scala appréciées\n",
    "Pratique des outils de versioning (Git, Bitbucket, Github)\n",
    "Expérience avec un outil ETL, idéalement Talend\n",
    "Une première approche de la dataviz (Tableau, QlikView) est un atout\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9611db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mots_cles: ['scala', 'pratiques', 'bitbucket', 'sql', 'garantir', 'bi', 'viendrez', 'modéliser', 'etudes', 'mission', 'didm', 'versioning', 'qlikview', 'études', 'poste', 'julia', 'dataviz', 'assurer', 'pourquoi', 'expérience', 'médicales', 'nouveau', 'valorisation', 'datasets', 'développement', 'support', 'r', 'appréciées', 'etl', 'sécurité', 'sous', 'connaissances', 'structurer', 'outil', 'maîtrise', 'périmètre', 'approche', 'résolution', 'compléter', 'cadre', 'pratique', 'suivantes', 'responsable', 'outils', 'qualité', 'nouveaux', 'bonnes', 'place', 'première', 'fiabilité', 'boîte', 'composée', 'croissant', 'pipelines', 'atout', 'face', 'mettre', 'tables', 'développer', 'fait', 'excellente', 'incidents', 'fiables', 'talend', 'besoin', 'responsabilité', 'standards', 'idéalement', 'schémas', 'oracle', 'missions', 'équipe', 'git', 'développements', 'java', 'créé', 'tableau', 'exploitation', 'alimenter', 'chargée', 'données', 'github', 'flux', 'dernier', 'analytique', 'construire', '50', 'solide']\n",
      "Score de correspondance : 46.6 %\n"
     ]
    }
   ],
   "source": [
    "# Test sur un CV individuel\n",
    "# Lis un CV .docx\n",
    "cv_text = lire_docx(\"test.docx\")\n",
    "\n",
    "score = score_cv_offre(cv_text, offre)\n",
    "print(f\"Score de correspondance : {score:.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb45c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de correspondance pour CV - Laurent D._14_01_2024.docx : 42.0 %\n",
      "Score de correspondance pour CV_AnaA_20250226.docx : 31.8 %\n",
      "Score de correspondance pour CV_CSA_NRJBI_20251016.docx : 28.4 %\n",
      "Score de correspondance pour NRJBI_CEC_CV_Senior.docx : 17.0 %\n",
      "Score de correspondance pour NRJBI_CV_EMO_202510_revisionElise.docx : 23.9 %\n",
      "Score de correspondance pour test.docx : 46.6 %\n"
     ]
    }
   ],
   "source": [
    "# Test sur plusieurs CV \n",
    "# Faire une boucle sur les CV dans le dossier CVs\n",
    "import os\n",
    "from docx import Document\n",
    "dossier_cvs = \"CVs\"\n",
    "for nom_fichier in os.listdir(dossier_cvs):\n",
    "    # Lis un CV .docx\n",
    "    cv_text = lire_docx(f\"./{dossier_cvs}/{nom_fichier}\")\n",
    "\n",
    "    score = score_cv_offre(cv_text, offre)\n",
    "    print(f\"Score de correspondance pour {nom_fichier} : {score:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9844dede",
   "metadata": {},
   "source": [
    "### Version tout le CV offre transformés en vecteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8eae0f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\eupho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# --- REQUIREMENTS ---\n",
    "# pip install scikit-learn nltk\n",
    "# ---------------------------------------------\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk, os, re\n",
    "from docx import Document\n",
    "\n",
    "# Téléchargement du stopword français\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_fr = stopwords.words('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c5d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Fonctions ---\n",
    "\n",
    "def lire_docx(path):\n",
    "    \"\"\"Extrait le texte brut d'un fichier Word (.docx).\"\"\"\n",
    "    doc = Document(path)\n",
    "    full_text = [p.text for p in doc.paragraphs]\n",
    "    return \"\\n\".join(full_text)\n",
    "\n",
    "def nettoyer_texte(texte: str) -> str:\n",
    "    \"\"\"Nettoie et normalise un texte français.\"\"\"\n",
    "    texte = texte.lower()\n",
    "    texte = re.sub(r\"[^a-zàâçéèêëîïôûùüÿñæœ0-9\\s]\", \" \", texte)\n",
    "    texte = re.sub(r\"\\s+\", \" \", texte)\n",
    "    texte = re.sub(r\"\\d+\", \" \", texte)  # supprime tous les nombres\n",
    "    return texte.strip()\n",
    "\n",
    "def score_cv_contre_offre(cv_text, offre_text):\n",
    "    \"\"\"Calcule le score TF-IDF entre un CV unique et l'offre.\"\"\"\n",
    "    # Nettoyage des textes\n",
    "    cv_clean = nettoyer_texte(cv_text)\n",
    "    offre_clean = nettoyer_texte(offre_text)\n",
    "\n",
    "    # TF-IDF\n",
    "    vectorizer = CountVectorizer(stop_words=stop_fr, ngram_range=(1,1))\n",
    "    count_words = vectorizer.fit_transform([offre_clean, cv_clean])\n",
    "\n",
    "    # Vecteurs\n",
    "    offre_vec = count_words[0:1]\n",
    "    cv_vec = count_words[1:2]\n",
    "\n",
    "    # Similarité cosinus\n",
    "    score = cosine_similarity(offre_vec, cv_vec)[0][0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d46e008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score de correspondance : 38.9 %\n"
     ]
    }
   ],
   "source": [
    "# Lis l'offre\n",
    "offre_text = \"\"\"Dans le cadre de sa mission d’exploitation et de valorisation des données médicales, \n",
    "la DIDM fait face à un besoin croissant de données fiables. Vous viendrez compléter une équipe...\n",
    "Excellente maîtrise de SQL (Oracle) et solide expérience en R...\n",
    "Pratique des outils de versioning (Git, Bitbucket, Github)\n",
    "Expérience avec un outil ETL, idéalement Talend\n",
    "Une première approche de la dataviz (Tableau, QlikView) est un atout.\"\"\"\n",
    "\n",
    "# Lis un CV .docx\n",
    "cv_text = lire_docx(\"test.docx\")\n",
    "\n",
    "# Calcul du score\n",
    "score = score_cv_contre_offre(cv_text, offre_text)\n",
    "print(f\"Score de correspondance : {score*100:.1f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41b39b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ingénieur Data / Développeur BI – Spécialisation données médicales\\nEmail : jean.dupont@example.com\\nTéléphone : 06 45 89 22 77\\n\\nProfil\\nData engineer expérimenté dans l’exploitation, la fiabilité et la valorisation de données médicales.\\nHabitué à modéliser des flux de données, construire des pipelines ETL complexes sous Talend et SQL (Oracle), et à assurer la qualité, la sécurité et la traçabilité des données pour des environnements analytiques et décisionnels (BI, tableaux de bord, reporting).\\n\\nCompétences clés\\nLangages : SQL (Oracle, PostgreSQL), R, Java, Scala, Julia (bases)\\nOutils ETL / Data Pipeline : Talend, Airflow, DataStage\\nBusiness Intelligence : Tableau, QlikView, Power BI\\nVersioning : Git, Bitbucket, GitHub\\nData Modeling : conception de schémas, modélisation de tables, intégration de données hétérogènes\\nQualité & Sécurité : validation des datasets, contrôles de cohérence, gestion des accès et anonymisation\\nBonnes pratiques : documentation, revue de code, standards de développement BI\\nAutres outils : Linux, Bash, Excel avancé, Jupyter, RStudio\\n\\nExpérience professionnelle\\n2021 – aujourd’hui : Data Engineer – Département d’Information et Données Médicales (DIDM), Hôpital Saint-Louis – Paris\\nConstruction de pipelines de données pour alimenter la BI et l’analytique.\\nModélisation et structuration de flux, tables et schémas de données médicales.\\nDéveloppement de datasets BI utilisés pour les tableaux de bord Tableau et QlikView.\\nMise en place de standards de développement et de bonnes pratiques (Git + Talend).\\nSupport et résolution d’incidents sur le périmètre data.\\nGarantir la fiabilité et la sécurité des données selon les contraintes CNIL et RGPD.\\n2018 – 2021 : Analyste Développeur BI – Laboratoires Santé France\\nDéveloppement d’ETL sous Talend et intégration Oracle.\\nModélisation de l’entrepôt de données (schéma en étoile).\\nTableaux de bord interactifs sous Tableau et QlikView.\\nNettoyage, validation et contrôle qualité des datasets.\\n\\nFormation\\nMaster Informatique – Spécialisation Data Engineering et BI (Université de Lyon)\\nCertification Talend Data Integration Advanced\\nCertification Oracle SQL Expert\\nCertification GitHub Foundations\\n\\nLangues\\nFrançais (natif)\\nAnglais (professionnel)\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c064943b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07637796, 0.27880457])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b9cd75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.3672651915581337\n"
     ]
    }
   ],
   "source": [
    "#Scoring CV hors fonction\n",
    "\"\"\"Calcule le score TF-IDF entre un CV unique et l'offre.\"\"\"\n",
    "# Nettoyage des textes\n",
    "cv_clean = nettoyer_texte(cv_text)\n",
    "offre_clean = nettoyer_texte(offre_text)\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = CountVectorizer(stop_words=stop_fr, ngram_range=(1,1))\n",
    "count_words = vectorizer.fit_transform([offre_clean, cv_clean])\n",
    "\n",
    "# Vecteurs\n",
    "offre_vec = count_words[0:1]\n",
    "cv_vec = count_words[1:2]\n",
    "\n",
    "# Similarité cosinus\n",
    "score = cosine_similarity(offre_vec, cv_vec)[0][0]\n",
    "print(\"score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f9abc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accès' 'advanced' 'airflow' 'alimenter' 'analyste' 'analytique'\n",
      " 'analytiques' 'anglais' 'anonymisation' 'approche' 'assurer' 'atout'\n",
      " 'aujourd' 'autres' 'avancé' 'bases' 'bash' 'besoin' 'bi' 'bitbucket'\n",
      " 'bonnes' 'bord' 'business' 'cadre' 'certification' 'clés' 'cnil' 'code'\n",
      " 'cohérence' 'com' 'complexes' 'compléter' 'compétences' 'conception'\n",
      " 'construction' 'construire' 'contraintes' 'contrôle' 'contrôles'\n",
      " 'croissant' 'data' 'datasets' 'datastage' 'dataviz' 'didm'\n",
      " 'documentation' 'données' 'dupont' 'décisionnels' 'département'\n",
      " 'développement' 'développeur' 'email' 'engineer' 'engineering' 'entrepôt'\n",
      " 'environnements' 'etl' 'example' 'excel' 'excellente' 'expert'\n",
      " 'exploitation' 'expérience' 'expérimenté' 'face' 'fait' 'fiabilité'\n",
      " 'fiables' 'flux' 'formation' 'foundations' 'france' 'français' 'garantir'\n",
      " 'gestion' 'git' 'github' 'habitué' 'hui' 'hétérogènes' 'hôpital'\n",
      " 'idéalement' 'incidents' 'information' 'informatique' 'ingénieur'\n",
      " 'integration' 'intelligence' 'interactifs' 'intégration' 'java' 'jean'\n",
      " 'julia' 'jupyter' 'laboratoires' 'langages' 'langues' 'linux' 'louis'\n",
      " 'lyon' 'master' 'maîtrise' 'mise' 'mission' 'modeling' 'modélisation'\n",
      " 'modéliser' 'médicales' 'natif' 'nettoyage' 'oracle' 'outil' 'outils'\n",
      " 'paris' 'pipeline' 'pipelines' 'place' 'postgresql' 'power' 'pratique'\n",
      " 'pratiques' 'première' 'professionnel' 'professionnelle' 'profil'\n",
      " 'périmètre' 'qlikview' 'qualité' 'reporting' 'revue' 'rgpd' 'rstudio'\n",
      " 'résolution' 'saint' 'santé' 'scala' 'schéma' 'schémas' 'selon' 'solide'\n",
      " 'sous' 'spécialisation' 'sql' 'standards' 'structuration' 'support'\n",
      " 'sécurité' 'tableau' 'tableaux' 'tables' 'talend' 'traçabilité'\n",
      " 'téléphone' 'université' 'utilisés' 'validation' 'valorisation'\n",
      " 'versioning' 'viendrez' 'équipe' 'étoile']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "934835c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dans le cadre de sa mission d exploitation et de valorisation des données médicales la didm fait face à un besoin croissant de données fiables vous viendrez compléter une équipe excellente maîtrise de sql oracle et solide expérience en r pratique des outils de versioning git bitbucket github expérience avec un outil etl idéalement talend une première approche de la dataviz tableau qlikview est un atout\n"
     ]
    }
   ],
   "source": [
    "print(offre_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff91f504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x1164 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 181 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offre_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6076a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x1164 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1285 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e792ce45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dans le cadre de sa mission d exploitation et de valorisation des données médicales la didm fait face à un besoin croissant de données fiables c est pourquoi un nouveau poste est créé vous viendrez compléter une équipe composée d une chargée d études et développements à 50 et d un responsable etudes et développements sous la responsabilité de ce dernier vos missions seront les suivantes construire des pipelines de données pour alimenter la bi et l analytique modéliser et structurer les flux tables et schémas garantir la qualité la fiabilité et la sécurité des données développer de nouveaux datasets pour la bi de la didm mettre en place des standards de développement et de bonnes pratiques assurer le support et la résolution des incidents sur votre périmètre votre boîte à outils excellente maîtrise de sql oracle et solide expérience en r connaissances en julia java ou scala appréciées pratique des outils de versioning git bitbucket github expérience avec un outil etl idéalement talend une première approche de la dataviz tableau qlikview est un atout',\n",
       " 'christopher eck senior ingénieur data nous contacter 06 14 15 10 85 jba nrjbi com présentation profil polyvalent technique fonctionnel avec une expertise sur l environnement data microsoft sql server ssis ssas synapse azure cloud power bi gestion d équipe technique 5 autres développeurs et référent technique pendant plus de 2 ans chez saint gobain compétences expériences clés formations certification power bi pl 300 ielts 8 0 formation data analyst ingénieur bi nrjbi oct 2019 déc 2019 diplôme ingénieur ece paris 2014 2017 détail des missions responsable data pour la migration de l ensemble de l architecture data du métier mobility sql server ssis server agent jobs vers la plateforme azure synapse avec optimisation de procédure stockée gestion des mep azure devops référent technique d une équipe de 5 développeurs évaluation technique analyse des packages et procédure stockées existantes onprem pour les traduire et optimiser dans une version synapse traduction de code abac sap en procédure stockée pour intégration de données sap dans azure sql développeur data création sur synapse de linked services datasets pipelines tables vues procédure stockées triggers pour intégrer sql server postgresql api fichier csv fichier parquet transformer et faire des calculs sur les données pour alimenter un datawarehouse propre et optimisé architecture data conception du process de concaténation entre les données historiques de sap les nouvelles données de tesseract et les données historiques de onprem avec mis en place de merge et de mapping variabilisés dans une nouvelle table responsable des mep mise en place de process et planning de mep avant l intégration d azure devops et de ci cd puis mise en place de process d équipe et de validation de recette avec azure devops responsable de la documentation technique traduction et conception de tous les documents techniques pour le projet de migration dans confluence référent technique pour 5 ingénieur data organisation d oto hebdomadaire aide technique pour débloquer des problèmes d optimisation ou de traduction revue de code avant mep responsable du run onprem et synapse résolution d incidents sur la partie onprem et synapse réalisation d un audit de la performance de la plateforme synapse d un autre projet suite à l audit mise en place d un projet d optimisation avec estimation de la durée du projet et de ses impacts sur la performance conception d un process avec un notebook pyspark pour lire des fichiers delta dans un deltalake azure et faire un load incrémentale dans une base externe sql serveur environnement sql server ssis synapse azure datawarehouse azure devops confluence jira business analyst et développeur bi pour l ensemble des métiers toyota du recueil des besoins métiers à l intégration des données jusqu à la création de rapport power bi responsable projet data recueil des besoins des problématiques métiers et définition des spécifications fonctionnelles demandé par le métier analyse de la donnée métier pour l intégrer au data model existant estimation de la durée du projet développeur bi intégration des données dans la base de données sql server dans le format stg ods dwh avec des packages ssis pour la transformation des données entre les différents storage intégration des nouvelles données dans le data modèle avec les jointures nécessaires création de kpi dans un cube ssas à l aide de formule dax développeur power bi création de rapport power bi ainsi que de template toyota pour l ensemble des rapports power bi de la société formateur formation d utilisateurs métiers à l utilisation de power bi environ 30 collaborateurs et des templates de la société responsable du run résolution d incidents sql ssis ssas et power bi pour l ensemble de l architecture data de toyota responsable du projet de nomenclature toyota définition de toutes les règles de nomenclatures pour les objets sql power bi et les formules dax pour uniformiser la vision totoya application de mise en place de la nouvelle nomenclature sur plus de 2000 kpi 1000 tables 200 packages environnement sql server ssis ssas power bi servicenow développeur microstrategy pour la recette de la migration des rapports qlikview vers microstrategy recette des données et des kpi d une trentaine de rapports migrés de qlikview vers microstrategy pour le chef de projet création et optimisation de visuels sur microstrategy pour correspondre aux visuels qlikview environnement microstrategy tableau snowflake sql responsable de l automatisation du suivi et de l analyse des tableaux de bords sociaux de l entreprise pour la direction et les drh du groupe lagardère active automatisation avec excel vba de la création et des calculs des rapports sociaux de l entreprise rse bilan social effectif masse salariale mobilités création et mise en place de maquette de calcul automatisées excel avec leur procédure d utilisation et la formation d utilisateurs pour fiabiliser le contrôle des données de la paie dns net à payer calcul part variable responsable du run pour l extraction la transformation et l analyse de données pour répondre aux besoins des différentes directions du groupe avec peoplesoft et excel générale rh finance contrôle de gestion environnement peoplesoft excel vba',\n",
       " 'ingénieur data développeur bi spécialisation données médicales email jean dupont example com téléphone 06 45 89 22 77 profil data engineer expérimenté dans l exploitation la fiabilité et la valorisation de données médicales habitué à modéliser des flux de données construire des pipelines etl complexes sous talend et sql oracle et à assurer la qualité la sécurité et la traçabilité des données pour des environnements analytiques et décisionnels bi tableaux de bord reporting compétences clés langages sql oracle postgresql r java scala julia bases outils etl data pipeline talend airflow datastage business intelligence tableau qlikview power bi versioning git bitbucket github data modeling conception de schémas modélisation de tables intégration de données hétérogènes qualité sécurité validation des datasets contrôles de cohérence gestion des accès et anonymisation bonnes pratiques documentation revue de code standards de développement bi autres outils linux bash excel avancé jupyter rstudio expérience professionnelle 2021 aujourd hui data engineer département d information et données médicales didm hôpital saint louis paris construction de pipelines de données pour alimenter la bi et l analytique modélisation et structuration de flux tables et schémas de données médicales développement de datasets bi utilisés pour les tableaux de bord tableau et qlikview mise en place de standards de développement et de bonnes pratiques git talend support et résolution d incidents sur le périmètre data garantir la fiabilité et la sécurité des données selon les contraintes cnil et rgpd 2018 2021 analyste développeur bi laboratoires santé france développement d etl sous talend et intégration oracle modélisation de l entrepôt de données schéma en étoile tableaux de bord interactifs sous tableau et qlikview nettoyage validation et contrôle qualité des datasets formation master informatique spécialisation data engineering et bi université de lyon certification talend data integration advanced certification oracle sql expert certification github foundations langues français natif anglais professionnel']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "331c7633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(cv_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc551d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aceaf001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dans le cadre de sa mission d’exploitation et de valorisation des données médicales, la DIDM fait face à un besoin croissant de données fiables. C’est pourquoi un nouveau poste est créé.\\nVous viendrez compléter une équipe composée d’une Chargée d’études et développements à 50 % et d’un Responsable Etudes et Développements. Sous la responsabilité de ce dernier, vos missions seront les suivantes :\\nConstruire des pipelines de données pour alimenter la BI et l’analytique.\\nModéliser et structurer les flux, tables et schémas\\nGarantir la qualité, la fiabilité et la sécurité des données\\nDévelopper de nouveaux datasets pour la BI de la DIDM\\nMettre en place des standards de développement et de bonnes pratiques\\nAssurer le support et la résolution des incidents sur votre périmètre...\\n\\nVotre boîte à outils\\nExcellente maîtrise de SQL (Oracle) et solide expérience en R\\nConnaissances en Julia, Java ou Scala appréciées\\nPratique des outils de versioning (Git, Bitbucket, Github)\\nExpérience avec un outil ETL, idéalement Talend\\nUne première approche de la dataviz (Tableau, QlikView) est un atout',\n",
       " '\\n\\n\\n\\nCHRISTOPHER ECK\\nSenior Ingénieur Data\\n\\n\\n\\n\\n\\n\\nNous contacter\\xa0:\\n 06.14.15.10.85\\n  jba@nrjbi.com\\n\\n  \\n\\n\\n\\n\\n\\n\\nPrésentation\\n\\nProfil polyvalent technique / fonctionnel, avec une expertise sur l’environnement data Microsoft (SQL Server, SSIS, SSAS, Synapse, Azure Cloud, Power BI). Gestion d’équipe technique (5 autres développeurs) et référent technique pendant plus de 2 ans chez Saint-Gobain.\\n\\n\\nCompétences\\n\\n\\nExpériences - Clés\\n\\n\\nFormations\\n\\nCertification Power BI PL-300\\n\\nIELTS (8.0)\\n\\nFormation Data Analyst / Ingénieur BI NRJBI (oct 2019 – déc 2019)\\n\\nDiplôme Ingénieur ECE Paris (2014-2017)\\n\\n\\nDétail des missions\\n\\n\\nResponsable data pour la Migration de l’ensemble de l’architecture data du métier Mobility (SQL server, SSIS, Server Agent Jobs) vers la plateforme Azure Synapse, avec optimisation de procédure stockée, gestion des MEP (Azure Devops), référent technique d’une équipe de 5 développeurs.\\n\\nÉvaluation technique : Analyse des packages et procédure stockées existantes OnPrem pour les traduire et optimiser dans une version Synapse. Traduction de code ABAC (SAP) en procédure stockée pour intégration de données SAP dans Azure SQL.\\n\\nDéveloppeur Data\\xa0: Création sur Synapse de Linked services, Datasets, Pipelines, Tables, Vues, Procédure stockées, Triggers pour intégrer (SQL Server, PostgreSQL, API, Fichier CSV, Fichier Parquet…), transformer et faire des calculs sur les données pour alimenter un datawarehouse propre et optimisé.\\n\\nArchitecture Data\\xa0: Conception du process de concaténation entre les données historiques de SAP, les nouvelles données de Tesseract et les données historiques de OnPrem avec mis en place de MERGE et de mapping variabilisés dans une nouvelle table.\\n\\nResponsable des MEP : Mise en place de process et planning de MEP avant l’intégration d’Azure DevOps et de CI/CD. Puis mise en place de process d’équipe et de validation de recette avec Azure DevOps.\\n\\nResponsable de la documentation technique : Traduction et conception de tous les documents techniques pour le projet de migration dans Confluence.\\n\\nRéférent technique pour 5 Ingénieur Data, organisation d’OTO hebdomadaire, aide technique pour débloquer des problèmes d’optimisation ou de traduction, revue de code avant MEP.\\n\\nResponsable du Run OnPrem et Synapse : Résolution d’incidents sur la partie OnPrem et Synapse.\\n\\nRéalisation d’un audit de la performance de la plateforme Synapse d’un autre projet. Suite à l’audit, mise en place d’un projet d’optimisation avec estimation de la durée du projet et de ses impacts sur la performance.\\n\\nConception d’un process avec un notebook Pyspark pour lire des fichiers delta dans un Deltalake Azure, et faire un load incrémentale dans une base externe SQL serveur.\\n\\nEnvironnement : SQL Server, SSIS, Synapse, Azure Datawarehouse, Azure DevOps, Confluence, Jira\\n\\n\\nBusiness Analyst et développeur BI pour l’ensemble des métiers Toyota du recueil des besoins métiers à l’intégration des données jusqu’à la création de rapport Power BI.\\n\\nResponsable projet Data : Recueil des besoins des problématiques métiers et définition des spécifications fonctionnelles demandé par le métier. Analyse de la donnée métier pour l’intégrer au data model existant. Estimation de la durée du projet.\\n\\nDéveloppeur BI\\xa0: Intégration des données dans la base de données SQL Server dans le format STG -> ODS -> DWH avec des packages SSIS pour la transformation des données entre les différents storage. Intégration des nouvelles données dans le data modèle avec les jointures nécessaires. Création de KPI dans un cube SSAS à l’aide de formule DAX.\\n\\nDéveloppeur Power BI\\xa0: Création de rapport Power BI ainsi que de template Toyota pour l’ensemble des rapports Power BI de la société.\\n\\nFormateur\\xa0: Formation d’utilisateurs métiers à l’utilisation de Power BI (environ 30 collaborateurs) et des templates de la société.\\n\\nResponsable du Run : Résolution d’incidents SQL, SSIS, SSAS et Power BI pour l’ensemble de l’architecture data de Toyota.\\n\\nResponsable du projet de nomenclature Toyota\\xa0: définition de toutes les règles de nomenclatures pour les objets SQL, Power BI et les formules DAX pour uniformiser la vision Totoya. Application de mise en place de la nouvelle nomenclature sur plus de 2000 KPI, 1000 tables, 200 packages. \\n\\nEnvironnement : SQL Server, SSIS, SSAS, Power BI, ServiceNow\\n\\n\\n\\nDéveloppeur MicroStrategy pour la recette de la Migration des rapports Qlikview vers MicroStrategy.\\n\\nRecette des données et des KPI d’une trentaine de rapports migrés de Qlikview vers MicroStrategy pour le chef de projet.\\n\\nCréation et optimisation de visuels sur MicroStrategy pour correspondre aux visuels Qlikview.\\n\\nEnvironnement : MicroStrategy, Tableau, Snowflake, SQL\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nResponsable de l’automatisation, du suivi et de l’analyse des tableaux de bords sociaux de l’entreprise pour la Direction et les DRH du groupe Lagardère Active.\\n\\nAutomatisation avec Excel VBA de la création et des calculs des rapports sociaux de l’entreprise (RSE, Bilan social, Effectif, Masse Salariale, Mobilités).\\n\\nCréation et mise en place de maquette de calcul automatisées Excel avec leur procédure d’utilisation et la formation d’utilisateurs pour fiabiliser le contrôle des données de la paie (DNS, Net à payer, Calcul part variable).\\n\\nResponsable du Run pour l’extraction, la transformation et l’analyse de données pour répondre aux besoins des différentes directions du Groupe avec PeopleSoft et Excel (Générale, RH, Finance, Contrôle de gestion).\\n\\nEnvironnement : PeopleSoft, Excel, VBA\\n\\n',\n",
       " 'Ingénieur Data / Développeur BI – Spécialisation données médicales\\nEmail : jean.dupont@example.com\\nTéléphone : 06 45 89 22 77\\n\\nProfil\\nData engineer expérimenté dans l’exploitation, la fiabilité et la valorisation de données médicales.\\nHabitué à modéliser des flux de données, construire des pipelines ETL complexes sous Talend et SQL (Oracle), et à assurer la qualité, la sécurité et la traçabilité des données pour des environnements analytiques et décisionnels (BI, tableaux de bord, reporting).\\n\\nCompétences clés\\nLangages : SQL (Oracle, PostgreSQL), R, Java, Scala, Julia (bases)\\nOutils ETL / Data Pipeline : Talend, Airflow, DataStage\\nBusiness Intelligence : Tableau, QlikView, Power BI\\nVersioning : Git, Bitbucket, GitHub\\nData Modeling : conception de schémas, modélisation de tables, intégration de données hétérogènes\\nQualité & Sécurité : validation des datasets, contrôles de cohérence, gestion des accès et anonymisation\\nBonnes pratiques : documentation, revue de code, standards de développement BI\\nAutres outils : Linux, Bash, Excel avancé, Jupyter, RStudio\\n\\nExpérience professionnelle\\n2021 – aujourd’hui : Data Engineer – Département d’Information et Données Médicales (DIDM), Hôpital Saint-Louis – Paris\\nConstruction de pipelines de données pour alimenter la BI et l’analytique.\\nModélisation et structuration de flux, tables et schémas de données médicales.\\nDéveloppement de datasets BI utilisés pour les tableaux de bord Tableau et QlikView.\\nMise en place de standards de développement et de bonnes pratiques (Git + Talend).\\nSupport et résolution d’incidents sur le périmètre data.\\nGarantir la fiabilité et la sécurité des données selon les contraintes CNIL et RGPD.\\n2018 – 2021 : Analyste Développeur BI – Laboratoires Santé France\\nDéveloppement d’ETL sous Talend et intégration Oracle.\\nModélisation de l’entrepôt de données (schéma en étoile).\\nTableaux de bord interactifs sous Tableau et QlikView.\\nNettoyage, validation et contrôle qualité des datasets.\\n\\nFormation\\nMaster Informatique – Spécialisation Data Engineering et BI (Université de Lyon)\\nCertification Talend Data Integration Advanced\\nCertification Oracle SQL Expert\\nCertification GitHub Foundations\\n\\nLangues\\nFrançais (natif)\\nAnglais (professionnel)\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
